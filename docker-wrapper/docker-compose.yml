# version: '3.9'

services:
  loghi-wrapper:
    build:
      context: ..
      dockerfile: ./docker-wrapper/Dockerfile
    image: loghi-wrapper:dind-gpu 
    container_name: loghi-wrapper
    privileged: true # For DinD

    # GPU access for the loghi-wrapper container itself from the host
    # This makes the host GPUs available to the loghi-wrapper.
    # The inner dockerd then needs to be configured to use them.
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all # Or specify a number, e.g., 1
              capabilities: [gpu]

    volumes:
      - loghi_dind_storage:/var/lib/docker # For the inner Docker daemon's data
      # Host mounts for your project code and data
      - ${LAYPA_MODULE:-./submodules/laypa}:/app/laypa
      - ${LOGHI_HTR_MODULE:-./submodules/loghi-htr}:/app/loghi-htr
      - ${LOGHI_TOOLING_MODULE:-./submodules/loghi-tooling}:/app/loghi-tooling
      - ${PRIMA_CORE_LIBS_MODULE:-./submodules/prima-core-libs}:/app/prima-core-libs
      - ${WORKSPACE_PATH:-./data/pipeline_input}:/workspace
      - ${DESTINATION_PATH:-./data/pipeline_output}:/destination
      # Named volumes for wrapper's internal use
      - loghi_wrapper_logs:/app/logs
      - loghi_wrapper_tmp:/tmp 
      - loghi_wrapper_app_temp:/app/temp_workspace

    environment:
      - TZ=${TZ:-Europe/Amsterdam}
      - WORKSPACE_PATH=/workspace         
      - DESTINATION_PATH=/destination   
      - LANG=en_US.UTF-8
      - LANGUAGE=en_US:en
      - LC_ALL=en_US.UTF-8
      - MPLCONFIGDIR=/tmp/matplotlib_cache_wrapper
      # Propagate NVIDIA environment variables needed for CUDA applications inside the wrapper
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility

    command: # Startup command for loghi-wrapper
      - sh
      - -c
      - |
          set -e;
          echo 'DinD GPU Setup: Starting internal Docker daemon service...';
          
          # Configure the inner Docker daemon for NVIDIA runtime
          echo 'DinD GPU Setup: Configuring inner Docker daemon (daemon.json) for NVIDIA runtime...';
          mkdir -p /etc/docker;
          # This daemon.json enables the nvidia-container-runtime
          # It assumes nvidia-container-runtime is installed and discoverable
          # Default runtime is still runc, but nvidia runtime is made available.
          # Tools must explicitly request it or it must be set as default.
          # For simplicity, we'll try to make it default if nvidia-container-runtime-hook is present.
          # A more robust way is to ensure nvidia-docker2 package also installs /etc/docker/daemon.json,
          # or to use nvidia-ctk runtime configure --runtime=docker --config=/etc/docker/daemon.json
          
          # Simple daemon.json configuration
          # If nvidia-container-runtime is available, this should allow --gpus flag to work
          cat <<EOF > /etc/docker/daemon.json
          {
              "runtimes": {
                  "nvidia": {
                      "path": "nvidia-container-runtime",
                      "runtimeArgs": []
                  }
              },
              "default-runtime": "nvidia" 
          }
          EOF
          echo "DinD GPU Setup: Inner daemon.json configured:";
          cat /etc/docker/daemon.json;

          # Start inner Docker daemon
          (dockerd --host=unix:///var/run/docker.sock --host=tcp://0.0.0.0:2375 > /var/log/dockerd.log 2>&1 &);
          
          echo 'DinD GPU Setup: Waiting for Docker daemon to be responsive...';
          timeout 60s sh -c 'while ! docker -H unix:///var/run/docker.sock info > /dev/null 2>&1; do echo -n "D_ping." ; sleep 1; done';
          echo; echo 'DinD GPU Setup: Internal Docker daemon is responsive.';
          
          echo 'DinD GPU Setup: Setting up directories and permissions...';
          mkdir -p /tmp/matplotlib_cache_wrapper && chmod -R 0777 /tmp/matplotlib_cache_wrapper;
          chmod 0777 /tmp;
          mkdir -p /app/logs && chmod -R 0777 /app/logs;
          mkdir -p /app/temp_workspace && chmod -R 0777 /app/temp_workspace;
          
          echo 'DinD GPU Setup: Chowning relevant directories to ubuntu user...';
          chown -R ubuntu:ubuntu /app /tmp /app/logs /app/temp_workspace /home/ubuntu;
          chown ubuntu:ubuntu /workspace || echo 'Warning: Could not chown /workspace';
          chown ubuntu:ubuntu /destination || echo 'Warning: Could not chown /destination';
          
          echo 'DinD GPU Setup: Wrapper container setup complete. Switching to ubuntu user for main process.';
          su ubuntu -c 'echo "DinD GPU User Context: Now running as user: $(whoami) (UID $(id -u), GID $(id -g))"; \
                         echo "DinD GPU User Context: NVIDIA SMI (should show GPUs if passthrough works):"; \
                         nvidia-smi || echo "nvidia-smi not found or failed (GPU passthrough to wrapper might be an issue)"; \
                         echo "DinD GPU User Context: Docker version (talking to internal daemon): $(docker --version)"; \
                         echo "DinD GPU User Context: Inner Docker Info (check runtimes):"; \
                         docker info || echo "Failed to get inner docker info"; \
                         echo "DinD GPU User Context: To run your pipeline, exec into this container as ubuntu and run ./workspace_na_pipeline.sh"; \
                         tail -f /var/log/dockerd.log /app/logs/pipeline_cron_runs.log /var/log/syslog'

volumes:
  loghi_dind_storage: {}       
  loghi_wrapper_logs: {}       
  loghi_wrapper_tmp: {}        
  loghi_wrapper_app_temp: {}
